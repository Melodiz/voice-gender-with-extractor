{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Suppress all warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/voice.csv', sep=',')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop('label', axis=1), data['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier()),\n",
    "    ('XGBoost', XGBClassifier()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Neural Network', MLPClassifier(max_iter=1000))\n",
    "]\n",
    "\n",
    "metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.8737 (0.0261)\n",
      "Support Vector Machine: 0.7194 (0.0145)\n",
      "Decision Tree: 0.9645 (0.0085)\n",
      "Random Forest: 0.9791 (0.0057)\n",
      "Gradient Boosting: 0.9759 (0.0042)\n",
      "XGBoost: 0.9799 (0.0055)\n",
      "K-Nearest Neighbors: 0.7861 (0.0143)\n",
      "Naive Bayes: 0.9313 (0.0063)\n",
      "Neural Network: 0.9716 (0.0046)\n",
      "\n",
      "Best Model: XGBoost, with an accuracy of 0.9799\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model using cross-validation\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring=metric)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {cv_results.mean():.4f} ({cv_results.std():.4f})\")\n",
    "\n",
    "# Select the best model based on cross-validation results\n",
    "best_model_index = np.argmax([result.mean() for result in results])\n",
    "best_model_name, best_model = models[best_model_index]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}, with an accuracy of {results[best_model_index].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing not relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "meanfun: 0.7186\n",
      "IQR: 0.0750\n",
      "sfm: 0.0343\n",
      "minfun: 0.0336\n",
      "maxdom: 0.0240\n",
      "sd: 0.0231\n",
      "sp.ent: 0.0159\n",
      "meanfreq: 0.0142\n",
      "skew: 0.0098\n",
      "modindx: 0.0095\n",
      "Q75: 0.0090\n",
      "meandom: 0.0085\n",
      "maxfun: 0.0084\n",
      "mode: 0.0083\n",
      "mindom: 0.0076\n"
     ]
    }
   ],
   "source": [
    "# print the importance of features using XGBoost\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "feature_importances = model.feature_importances_\n",
    "sorted_feature_importances = sorted(zip(feature_importances, X_train.columns), reverse=True)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for importance, feature in sorted_feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the least important features (<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the least important features (<0.01)\n",
    "selected_features = [feature for importance, feature in sorted_feature_importances if importance >= 0.01]\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model again with the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.8927 (0.0131)\n",
      "Support Vector Machine: 0.6693 (0.0150)\n",
      "Decision Tree: 0.9692 (0.0084)\n",
      "Random Forest: 0.9799 (0.0047)\n",
      "Gradient Boosting: 0.9767 (0.0049)\n",
      "XGBoost: 0.9795 (0.0069)\n",
      "K-Nearest Neighbors: 0.8816 (0.0138)\n",
      "Naive Bayes: 0.9373 (0.0051)\n",
      "Neural Network: 0.9736 (0.0041)\n",
      "\n",
      "Best Model: Random Forest, with an accuracy of 0.9799\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model using cross-validation\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring=metric)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {cv_results.mean():.4f} ({cv_results.std():.4f})\")\n",
    "\n",
    "# Select the best model based on cross-validation results\n",
    "best_model_index = np.argmax([result.mean() for result in results])\n",
    "best_model_name, best_model = models[best_model_index]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}, with an accuracy of {results[best_model_index].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.9665 (0.0096)\n",
      "Support Vector Machine: 0.9672 (0.0106)\n",
      "Decision Tree: 0.9669 (0.0063)\n",
      "Random Forest: 0.9787 (0.0064)\n",
      "Gradient Boosting: 0.9791 (0.0042)\n",
      "XGBoost: 0.9807 (0.0072)\n",
      "K-Nearest Neighbors: 0.8931 (0.0129)\n",
      "Naive Bayes: 0.9373 (0.0051)\n",
      "Neural Network: 0.9696 (0.0090)\n",
      "\n",
      "Best Model: XGBoost, with an accuracy of 0.9807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "    'Support Vector Machine': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'Decision Tree': {'max_depth': [10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [10, 50, 100], 'max_features': ['auto', 'sqrt', 'log2']},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n",
    "    'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n",
    "    'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7, 9]},\n",
    "    'Naive Bayes': {},\n",
    "    'Neural Network': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'alpha': [0.0001, 0.001, 0.01]}\n",
    "}\n",
    "\n",
    "# Evaluate each model using Grid Search\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    param_grid = param_grids[name]\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=metric)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    cv_results = cross_val_score(best_model, X_train, y_train, cv=5, scoring=metric)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {cv_results.mean():.4f} ({cv_results.std():.4f})\")\n",
    "\n",
    "# Select the best model based on cross-validation results\n",
    "best_model_index = np.argmax([result.mean() for result in results])\n",
    "best_model_name, best_model = models[best_model_index]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}, with an accuracy of {results[best_model_index].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
